### Tools
1. mytokenizer.py (and its files in /lib) is the [tokenizer by YeDeheng](https://github.com/YeDeheng/s-tokenizer) adapted for Python 3.6.5.
2. The other present modules use a customized [spaCy](https://spacy.io/) tokenizer for corpus preprocessing and sentence splitting for text containing software terms (Java).