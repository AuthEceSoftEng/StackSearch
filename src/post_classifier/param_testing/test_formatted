#accuracy batch_size epochs
0.939500 40 5
0.912500 40 6
0.923000 40 7
0.932000 60 5
0.920000 60 6
0.929500 60 7
0.909500 80 5
0.935000 80 6
0.936000 80 7
0.934500 100 5
0.937500 100 6
0.916000 100 7
0.876000 120 5
0.937500 120 6
0.930500 120 7 
#accuracy dropout recurrent_dropout
0.931500 0.4 0.05
0.942000 0.4 0.1
0.939000 0.4 0.15
0.942500 0.4 0.2
0.935500 0.45 0.05
0.889000 0.45 0.1
0.941000 0.45 0.15
0.940500 0.45 0.2
0.938500 0.5 0.05
0.921500 0.5 0.1
0.940000 0.5 0.15
0.935000 0.5 0.2
0.930000 0.55 0.05
0.929000 0.55 0.1
0.935500 0.55 0.15
0.931000 0.55 0.2
0.944000 0.6 0.05
0.941500 0.6 0.1
0.916000 0.6 0.15
0.927500 0.6 0.2
#accuracy optimizer
0.720500 SGD
0.939500 RMSprop
0.945500 Adagrad
0.931500 Adadelta
0.940000 Adam
0.853500 Adamax
0.935500 Nadam
#accuracy activation
0.871500 softmax
0.780000 softplus
0.925000 softsign
0.939000 relu
0.935000 tanh
0.859500 sigmoid
0.816500 hard_sigmoid
0.909000 linear
#accuracy embed_len
0.909000 20
0.902000 25
0.909500 30
0.940500 35
0.933500 40
0.935000 45
0.935500 50
0.938500 55
0.936500 60
0.922500 65
0.926000 70
0.936500 75
